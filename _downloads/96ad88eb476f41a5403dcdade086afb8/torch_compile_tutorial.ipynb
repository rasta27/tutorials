{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# torch.compile Tutorial\n**Author:** William Wen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``torch.compile`` is the latest method to speed up your PyTorch code!\n``torch.compile`` makes PyTorch code run faster by\nJIT-compiling PyTorch code into optimized kernels,\nall while requiring minimal code changes.\n\nIn this tutorial, we cover basic ``torch.compile`` usage,\nand demonstrate the advantages of ``torch.compile`` over\nprevious PyTorch compiler solutions, such as\n[TorchScript](https://pytorch.org/docs/stable/jit.html)_ and \n[FX Tracing](https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace)_.\n\n**Contents**\n\n- Basic Usage\n- Demonstrating Speedups\n- Comparison to TorchScript and FX Tracing\n- TorchDynamo and FX Graphs\n- Conclusion\n\n**Required pip Dependencies**\n\n- ``torch >= 2.0``\n- ``torchvision``\n- ``numpy``\n- ``scipy``\n- ``tabulate``\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE: a modern NVIDIA GPU (H100, A100, or V100) is recommended for this tutorial in\norder to reproduce the speedup numbers shown below and documented elsewhere.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport warnings\n\ngpu_ok = False\nif torch.cuda.is_available():\n    device_cap = torch.cuda.get_device_capability()\n    if device_cap in ((7, 0), (8, 0), (9, 0)):\n        gpu_ok = True\n\nif not gpu_ok:\n    warnings.warn(\n        \"GPU is not NVIDIA V100, A100, or H100. Speedup numbers may be lower \"\n        \"than expected.\"\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Usage\n\n``torch.compile`` is included in the latest PyTorch..\nRunning TorchInductor on GPU requires Triton, which is included with the PyTorch 2.0 nightly\nbinary. If Triton is still missing, try installing ``torchtriton`` via pip \n(``pip install torchtriton --extra-index-url \"https://download.pytorch.org/whl/nightly/cu117\"``\nfor CUDA 11.7).\n\nArbitrary Python functions can be optimized by passing the callable to\n``torch.compile``. We can then call the returned optimized\nfunction in place of the original function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def foo(x, y):\n    a = torch.sin(x)\n    b = torch.cos(y)\n    return a + b\nopt_foo1 = torch.compile(foo)\nprint(opt_foo1(torch.randn(10, 10), torch.randn(10, 10)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, we can decorate the function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@torch.compile\ndef opt_foo2(x, y):\n    a = torch.sin(x)\n    b = torch.cos(y)\n    return a + b\nprint(opt_foo2(torch.randn(10, 10), torch.randn(10, 10)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also optimize ``torch.nn.Module`` instances.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = torch.nn.Linear(100, 10)\n\n    def forward(self, x):\n        return torch.nn.functional.relu(self.lin(x))\n\nmod = MyModule()\nopt_mod = torch.compile(mod)\nprint(opt_mod(torch.randn(10, 100)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demonstrating Speedups\n\nLet's now demonstrate that using ``torch.compile`` can speed\nup real models. We will compare standard eager mode and \n``torch.compile`` by evaluating and training a ``torchvision`` model on random data.\n\nBefore we start, we need to define some utility functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Returns the result of running `fn()` and the time it took for `fn()` to run,\n# in seconds. We use CUDA events and synchronization for the most accurate\n# measurements.\ndef timed(fn):\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    torch.cuda.synchronize()\n    return result, start.elapsed_time(end) / 1000\n\n# Generates random input and targets data for the model, where `b` is\n# batch size.\ndef generate_data(b):\n    return (\n        torch.randn(b, 3, 128, 128).to(torch.float32).cuda(),\n        torch.randint(1000, (b,)).cuda(),\n    )\n\nN_ITERS = 10\n\nfrom torchvision.models import densenet121\ndef init_model():\n    return densenet121().to(torch.float32).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's compare inference.\n\nNote that in the call to ``torch.compile``, we have have the additional\n``mode`` argument, which we will discuss below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate(mod, inp):\n    return mod(inp)\n\nmodel = init_model()\n\n# Reset since we are using a different mode.\nimport torch._dynamo\ntorch._dynamo.reset()\n\nevaluate_opt = torch.compile(evaluate, mode=\"reduce-overhead\")\n\ninp = generate_data(16)[0]\nprint(\"eager:\", timed(lambda: evaluate(model, inp))[1])\nprint(\"compile:\", timed(lambda: evaluate_opt(model, inp))[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that ``torch.compile`` takes a lot longer to complete\ncompared to eager. This is because ``torch.compile`` compiles\nthe model into optimized kernels as it executes. In our example, the\nstructure of the model doesn't change, and so recompilation is not\nneeded. So if we run our optimized model several more times, we should\nsee a significant improvement compared to eager.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eager_times = []\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)[0]\n    _, eager_time = timed(lambda: evaluate(model, inp))\n    eager_times.append(eager_time)\n    print(f\"eager eval time {i}: {eager_time}\")\n\nprint(\"~\" * 10)\n\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)[0]\n    _, compile_time = timed(lambda: evaluate_opt(model, inp))\n    compile_times.append(compile_time)\n    print(f\"compile eval time {i}: {compile_time}\")\nprint(\"~\" * 10)\n\nimport numpy as np\neager_med = np.median(eager_times)\ncompile_med = np.median(compile_times)\nspeedup = eager_med / compile_med\nprint(f\"(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\nprint(\"~\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And indeed, we can see that running our model with ``torch.compile``\nresults in a significant speedup. Speedup mainly comes from reducing Python overhead and\nGPU read/writes, and so the observed speedup may vary on factors such as model\narchitecture and batch size. For example, if a model's architecture is simple\nand the amount of data is large, then the bottleneck would be\nGPU compute and the observed speedup may be less significant.\n\nYou may also see different speedup results depending on the chosen ``mode``\nargument. Since our model and data are small, we want to reduce overhead as\nmuch as possible, and so we chose ``\"reduce-overhead\"``. For your own models,\nyou may need to experiment with different modes to maximize speedup. You can\nread more about modes [here](https://pytorch.org/get-started/pytorch-2.0/#user-experience)_.\n\nFor general PyTorch benchmarking, you can try using ``torch.utils.benchmark`` instead of the ``timed``\nfunction we defined above. We wrote our own timing function in this tutorial to show\n``torch.compile``'s compilation latency.\n\nNow, let's consider comparing training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = init_model()\nopt = torch.optim.Adam(model.parameters())\n\ndef train(mod, data):\n    opt.zero_grad(True)\n    pred = mod(data[0])\n    loss = torch.nn.CrossEntropyLoss()(pred, data[1])\n    loss.backward()\n    opt.step()\n\neager_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, eager_time = timed(lambda: train(model, inp))\n    eager_times.append(eager_time)\n    print(f\"eager train time {i}: {eager_time}\")\nprint(\"~\" * 10)\n\nmodel = init_model()\nopt = torch.optim.Adam(model.parameters())\ntrain_opt = torch.compile(train, mode=\"reduce-overhead\")\n\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, compile_time = timed(lambda: train_opt(model, inp))\n    compile_times.append(compile_time)\n    print(f\"compile train time {i}: {compile_time}\")\nprint(\"~\" * 10)\n\neager_med = np.median(eager_times)\ncompile_med = np.median(compile_times)\nspeedup = eager_med / compile_med\nprint(f\"(train) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\nprint(\"~\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, we can see that ``torch.compile`` takes longer in the first\niteration, as it must compile the model, but in subsequent iterations, we see\nsignificant speedups compared to eager.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison to TorchScript and FX Tracing\n\nWe have seen that ``torch.compile`` can speed up PyTorch code.\nWhy else should we use ``torch.compile`` over existing PyTorch\ncompiler solutions, such as TorchScript or FX Tracing? Primarily, the\nadvantage of ``torch.compile`` lies in its ability to handle\narbitrary Python code with minimal changes to existing code.\n\nOne case that ``torch.compile`` can handle that other compiler\nsolutions struggle with is data-dependent control flow (the \n``if x.sum() < 0:`` line below).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def f1(x, y):\n    if x.sum() < 0:\n        return -y\n    return y\n\n# Test that `fn1` and `fn2` return the same result, given\n# the same arguments `args`. Typically, `fn1` will be an eager function\n# while `fn2` will be a compiled function (torch.compile, TorchScript, or FX graph).\ndef test_fns(fn1, fn2, args):\n    out1 = fn1(*args)\n    out2 = fn2(*args)\n    return torch.allclose(out1, out2)\n\ninp1 = torch.randn(5, 5)\ninp2 = torch.randn(5, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TorchScript tracing ``f1`` results in\nsilently incorrect results, since only the actual control flow path\nis traced.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "traced_f1 = torch.jit.trace(f1, (inp1, inp2))\nprint(\"traced 1, 1:\", test_fns(f1, traced_f1, (inp1, inp2)))\nprint(\"traced 1, 2:\", test_fns(f1, traced_f1, (-inp1, inp2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FX tracing ``f1`` results in an error due to the presence of\ndata-dependent control flow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import traceback as tb\ntry:\n    torch.fx.symbolic_trace(f1)\nexcept:\n    tb.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we provide a value for ``x`` as we try to FX trace ``f1``, then\nwe run into the same problem as TorchScript tracing, as the data-dependent\ncontrol flow is removed in the traced function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fx_f1 = torch.fx.symbolic_trace(f1, concrete_args={\"x\": inp1})\nprint(\"fx 1, 1:\", test_fns(f1, fx_f1, (inp1, inp2)))\nprint(\"fx 1, 2:\", test_fns(f1, fx_f1, (-inp1, inp2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can see that ``torch.compile`` correctly handles\ndata-dependent control flow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reset since we are using a different mode.\ntorch._dynamo.reset()\n\ncompile_f1 = torch.compile(f1)\nprint(\"compile 1, 1:\", test_fns(f1, compile_f1, (inp1, inp2)))\nprint(\"compile 1, 2:\", test_fns(f1, compile_f1, (-inp1, inp2)))\nprint(\"~\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TorchScript scripting can handle data-dependent control flow, but this\nsolution comes with its own set of problems. Namely, TorchScript scripting\ncan require major code changes and will raise errors when unsupported Python\nis used.\n\nIn the example below, we forget TorchScript type annotations and we receive\na TorchScript error because the input type for argument ``y``, an ``int``,\ndoes not match with the default argument type, ``torch.Tensor``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def f2(x, y):\n    return x + y\n\ninp1 = torch.randn(5, 5)\ninp2 = 3\n\nscript_f2 = torch.jit.script(f2)\ntry:\n    script_f2(inp1, inp2)\nexcept:\n    tb.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, ``torch.compile`` is easily able to handle ``f2``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "compile_f2 = torch.compile(f2)\nprint(\"compile 2:\", test_fns(f2, compile_f2, (inp1, inp2)))\nprint(\"~\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another case that ``torch.compile`` handles well compared to\nprevious compilers solutions is the usage of non-PyTorch functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import scipy\ndef f3(x):\n    x = x * 2\n    x = scipy.fft.dct(x.numpy())\n    x = torch.from_numpy(x)\n    x = x * 2\n    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TorchScript tracing treats results from non-PyTorch function calls\nas constants, and so our results can be silently wrong.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inp1 = torch.randn(5, 5)\ninp2 = torch.randn(5, 5)\ntraced_f3 = torch.jit.trace(f3, (inp1,))\nprint(\"traced 3:\", test_fns(f3, traced_f3, (inp2,)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TorchScript scripting and FX tracing disallow non-PyTorch function calls.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    torch.jit.script(f3)\nexcept:\n    tb.print_exc()\n\ntry:\n    torch.fx.symbolic_trace(f3)\nexcept:\n    tb.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In comparison, ``torch.compile`` is easily able to handle\nthe non-PyTorch function call.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "compile_f3 = torch.compile(f3)\nprint(\"compile 3:\", test_fns(f3, compile_f3, (inp2,)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TorchDynamo and FX Graphs\n\nOne important component of ``torch.compile`` is TorchDynamo.\nTorchDynamo is responsible for JIT compiling arbitrary Python code into\n[FX graphs](https://pytorch.org/docs/stable/fx.html#torch.fx.Graph)_, which can\nthen be further optimized. TorchDynamo extracts FX graphs by analyzing Python bytecode\nduring runtime and detecting calls to PyTorch operations.\n\nNormally, TorchInductor, another component of ``torch.compile``,\nfurther compiles the FX graphs into optimized kernels,\nbut TorchDynamo allows for different backends to be used. In order to inspect\nthe FX graphs that TorchDynamo outputs, let us create a custom backend that\noutputs the FX graph and simply returns the graph's unoptimized forward method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import List\ndef custom_backend(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"custom backend called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward\n\n# Reset since we are using a different backend.\ntorch._dynamo.reset()\n\nopt_model = torch.compile(init_model(), backend=custom_backend)\nopt_model(generate_data(16)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using our custom backend, we can now see how TorchDynamo is able to handle\ndata-dependent control flow. Consider the function below, where the line\n``if b.sum() < 0`` is the source of data-dependent control flow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def bar(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\n\nopt_bar = torch.compile(bar, backend=custom_backend)\ninp1 = torch.randn(10)\ninp2 = torch.randn(10)\nopt_bar(inp1, inp2)\nopt_bar(inp1, -inp2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output reveals that TorchDynamo extracted 3 different FX graphs\ncorresponding the following code (order may differ from the output above):\n\n1. ``x = a / (torch.abs(a) + 1)``\n2. ``b = b * -1; return x * b``\n3. ``return x * b``\n\nWhen TorchDynamo encounters unsupported Python features, such as data-dependent\ncontrol flow, it breaks the computation graph, lets the default Python\ninterpreter handle the unsupported code, then resumes capturing the graph.\n\nLet's investigate by example how TorchDynamo would step through ``bar``.\nIf ``b.sum() < 0``, then TorchDynamo would run graph 1, let\nPython determine the result of the conditional, then run\ngraph 2. On the other hand, if ``not b.sum() < 0``, then TorchDynamo\nwould run graph 1, let Python determine the result of the conditional, then\nrun graph 3.\n\nThis highlights a major difference between TorchDynamo and previous PyTorch\ncompiler solutions. When encountering unsupported Python features,\nprevious solutions either raise an error or silently fail.\nTorchDynamo, on the other hand, will break the computation graph.\n\nWe can see where TorchDynamo breaks the graph by using ``torch._dynamo.explain``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reset since we are using a different backend.\ntorch._dynamo.reset()\nexplanation, out_guards, graphs, ops_per_graph, break_reasons, explanation_verbose = torch._dynamo.explain(\n    bar, torch.randn(10), torch.randn(10)\n)\nprint(explanation_verbose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to maximize speedup, graph breaks should be limited.\nWe can force TorchDynamo to raise an error upon the first graph\nbreak encountered by using ``fullgraph=True``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt_bar = torch.compile(bar, fullgraph=True)\ntry:\n    opt_bar(torch.randn(10), torch.randn(10))\nexcept:\n    tb.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And below, we demonstrate that TorchDynamo does not break the graph on\nthe model we used above for demonstrating speedups.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt_model = torch.compile(init_model(), fullgraph=True)\nprint(opt_model(generate_data(16)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, if we simply want TorchDynamo to output the FX graph for export,\nwe can use ``torch._dynamo.export``. Note that ``torch._dynamo.export``, like\n``fullgraph=True``, raises an error if TorchDynamo breaks the graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    torch._dynamo.export(bar, torch.randn(10), torch.randn(10))\nexcept:\n    tb.print_exc()\n\nmodel_exp = torch._dynamo.export(init_model(), generate_data(16)[0])\nprint(model_exp[0](generate_data(16)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial, we introduced ``torch.compile`` by covering\nbasic usage, demonstrating speedups over eager mode, comparing to previous\nPyTorch compiler solutions, and briefly investigating TorchDynamo and its interactions\nwith FX graphs. We hope that you will give ``torch.compile`` a try!\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}